{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "DigitalGlobe's GBDX platform provides customers with a fast and easy way to search, order, and process imagery. This tutorial is intended to demonstrate how to access GBDX APIs via the Python SDK, gbdxtools. Specifically, this tutorial will cover:\n",
    "\n",
    "1. [Import GBDXtools and authenticate credentials](#1.-Import-GBDXtools-and-authenticate-credentials)\n",
    "2. [Catalog API](#2.-Catalog-API)\n",
    "3. [Ordering API](#3.-Ordering-API)\n",
    "4. [Workflow API](#4.-Workflow-API)\n",
    "5. [Manage Workflows](#5.-Manage-Workflows)\n",
    "6. [A more complex Workflow](#6.-A-more-complex-Workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import GBDXtools and authenticate credentials \n",
    "__1.1 Run the code in a cell by first selecting it and then either clicking the play button in the toolbar, or using the keyboard shortcut shift + enter. Import the 'sys' library so you can check your Python instance, and 'json' so you can print output in an easy to read format. The print statement will return your Python instance.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "print (sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2 Fill in your your GBDX username, password, client ID and client secret in the following cell. This information can be found under your Profile information at https://gbdx.geobigdata.io/profile. If you have a GBDX config file, you can uncomment and use the first two lines of code to authenticate into GBDX.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gbdxtools import Interface\n",
    "# gbdx = Interface\n",
    "\n",
    "import gbdxtools\n",
    "gbdx = gbdxtools.Interface(\n",
    "    username='',\n",
    "    password='',\n",
    "    client_id='',\n",
    "    client_secret='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Catalog API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6432d41c-9bbd-4319-bca6-7bd46bcd9aa5"
    }
   },
   "source": [
    "The Catalog API is the backbone of data discovery through the APIs and allows you to search the all of the records and metadata contained in the DigitalGlobe archive, by geographic area, or parameters such as acquisition data, sensor, cloud cover, etc.\n",
    "\n",
    "__2.1 First, search the catalog by spatial area, as defined by a WKT polygon. All imagery that intersects the polygon will be returned, but we'll just look at one result.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wkt_string = \"POLYGON((151.247484595670215 -33.956915138583831, 151.247484595670215 -33.941147704639356, 151.266492160171651 -33.941147704639356, 151.266492160171651 -33.956915138583831,151.247484595670215 -33.956915138583831))\"\n",
    "\n",
    "results = gbdx.catalog.search(searchAreaWkt=wkt_string)\n",
    "print len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b54db840-6317-4f60-8aca-2bd1c942972b"
    }
   },
   "outputs": [],
   "source": [
    "print json.dumps(results[0:5], sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of interesting information here, such as image bands, image resolution, cloud cover, timestamp, etc. If you would like to learn more about the image metadata, check the documentation at   http://gbdxdocs.digitalglobe.com/docs/catalog-v2-record-metadata.\n",
    "\n",
    "One key metadata value to note is the Catalog ID of the image, which is unique to that image. When you order imagery to GBDX, you'll need to order it via its Catalog ID. \n",
    "\n",
    "```json\n",
    "\"catalogID\": \"10400100290BFF00\",\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "43c62228-cb8c-4f62-98d5-f6a1b0d6bca1"
    }
   },
   "source": [
    "__2.2 Add a start and end date to your search to filter the results by date range, compare the number of filtered results to the original search results.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c43c2ebc-0a31-47a6-86ae-b19a97cb493c"
    }
   },
   "outputs": [],
   "source": [
    "results = gbdx.catalog.search(searchAreaWkt=wkt_string,\n",
    "                              startDate=\"2016-09-08T00:00:00.000Z\",\n",
    "                              endDate=\"2017-03-08T23:59:59.999Z\")\n",
    "print len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "908be6fe-5c8f-477f-8ddd-d87b3d0503e3"
    }
   },
   "source": [
    "__2.3 Last, filter your search results by specific metadata values. Add filters for cloud cover, off-nadir angle, and image bands, and look at the results.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1947009c-2ab6-48ff-a6f1-606fd69dafbb"
    }
   },
   "outputs": [],
   "source": [
    "filters = [\n",
    "        \"cloudCover < 10\",\n",
    "        \"offNadirAngle < 15\",\n",
    "        \"imageBands = 'Pan_MS1_MS2'\"\n",
    "]\n",
    "\n",
    "results = gbdx.catalog.search(searchAreaWkt=wkt_string,\n",
    "                              startDate=\"2016-09-08T00:00:00.000Z\",\n",
    "                              endDate=\"2017-03-08T23:59:59.999Z\",\n",
    "                              filters=filters)\n",
    "print len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8a575bf2-bd7d-446b-b9b8-776245cd5f66"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print json.dumps(results, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5cf881d6-7ac2-4931-a2ea-2e82b794178d"
    }
   },
   "source": [
    "__ 2.4 Given the Catalog ID for an image, fetch its metadata.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7134f61b-a7be-4296-b68b-45a24d814913"
    }
   },
   "outputs": [],
   "source": [
    "record = gbdx.catalog.get('10400100245B7800')\n",
    "print json.dumps(record, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "23f1831e-0b15-4133-8f90-98bbc3266fd0"
    }
   },
   "source": [
    "## 3. Ordering API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagery will often need to first be ordered to GBDX before it can be processed. The GBDX Orders API lets you order imagery by catalog ID and check the status of your order. Learn more about Ordering at http://gbdxdocs.digitalglobe.com/docs/ordering-course-v2.\n",
    "\n",
    "__ 3.1 Define the Catalog ID to be ordered.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_ids = ['10400100245B7800']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to consider whether the image is available for processing on S3. If not, it must be ordered. In the following code, we will order an image by its catalog ID, which will return an order ID. We can then use that Order ID to check the status of the order, whether it's delivered and its S3 location.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 3.2 Order the image by its Catalog ID, return and print the order status. Because this image has already been ordered to GBDX, its 'state' is 'delivered' and the status call returns its location on S3.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9b4da4bb-5515-4a9a-8954-e48d69ee0da7"
    }
   },
   "outputs": [],
   "source": [
    "order_id = gbdx.ordering.order(cat_ids)\n",
    "order_status = gbdx.ordering.status(order_id)\n",
    "\n",
    "print json.dumps(order_status, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 3.3 You can order several catalog IDs in this way, up to 100. Use the same format as above to order three Catalog IDs.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f9fadea2-b929-4a46-b1ff-0b3c1e6d5d1d"
    }
   },
   "outputs": [],
   "source": [
    "cat_ids = ['101001000350E300', '104001001970EA00', '1040010011634E00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7aa5448b-d92a-4329-8834-a773a0733504"
    }
   },
   "outputs": [],
   "source": [
    "order_id = gbdx.ordering.order(cat_ids)\n",
    "order_status = gbdx.ordering.status(order_id)\n",
    "\n",
    "print json.dumps(order_status, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf05694d-036c-4b49-bd18-d2e299afe9b3"
    }
   },
   "source": [
    "## 4. Workflow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e2b72145-80ae-4e39-ad91-9e3063ef82dc"
    }
   },
   "source": [
    "A \"workflow\" is a series of tasks chained together to run on the GBDX platform. Each \"task\" is an individual process that performs a specific action against data, of which the inputs and outputs must be through S3. The outputs of one task are frequently the inputs to another.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "The first step to building a Workflow is to set up the individual Tasks that make up the Workflow. Each Task needs to be defined with its registered Task name, and assigned inputs. You can find documentation on many of the Tasks you'll use under 'Certified Algorithms' at http://gbdxdocs.digitalglobe.com/docs. However, it is also easy to interact with the Task object to get more information about it. \n",
    "\n",
    "The first Task in almost any Workflow is the Advanced Image Preprocessor Task, which orthorectifies raw imagery and offers other image pre-processing options. Documentation at https://gbdxdocs.digitalglobe.com/docs/advanced-image-preprocessor.\n",
    "\n",
    "__4.1 Define the Advanced Image Preprocessor task using its registered Task name, 'AOP_Strip_Processor', and print out the task definition.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aop_task = gbdx.Task(\"AOP_Strip_Processor\")\n",
    "print json.dumps(aop_task.definition, sort_keys=True, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task definition has a lot of useful information, including descriptions of inputs and outputs, and which of the inputs are required.\n",
    "\n",
    "__ 4.2 Run the code in the following cell to get a list of inputs for this Task.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print aop_task.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 4.3 Append an input port name to the same call to get detailed information about that input.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print aop_task.inputs.enable_acomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 4.4 Run the following code cells to find out more information about the Task outputs.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aop_task.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aop_task.outputs.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "00ef9d71-2e79-4b44-8db5-bc9dda3288c5"
    }
   },
   "source": [
    "___\n",
    "### Workflows\n",
    "Now that you know about Tasks, and about inputs and outputs to a Task, let's walk through the steps to building a Workflow. You will get a chance to try it out at the end of this explanation.  \n",
    "\n",
    "*S3 inputs and outputs*\n",
    "\n",
    "> The input data for a Task must be located on S3. This could be in the way of an image that you've ordered to GBDX, it could be data that you've staged in your S3 bucket, or it could be the output from a previous Task in the Workflow. For this example Workflow, you're going to use an image ordered to GBDX as input to the Task. \n",
    "\n",
    "```python\n",
    "# define the S3 location of the input data\n",
    "source_s3 = gbdx.catalog.get_data_location(catalog_id='10400100245B7800')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b792072e-3545-44e8-83ce-2aacceb39fe0"
    }
   },
   "source": [
    "> Just as the input must come from an S3 location, the output of a Task must be saved to an S3 location. Gbdxtools has a 'savedata' feature that will automatically save the output to your Customer S3 bucket, under the directory that you specify. We will cover the 'savedata' feature later in the script.\n",
    " \n",
    "```python\n",
    "# define the S3 location of the output directory\n",
    "target_s3 = 'demo_output/aop_10400100245B7800/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ea7ee1fc-8951-444c-a401-256df29ef2ec"
    }
   },
   "source": [
    "*Defining Task(s) *\n",
    "\n",
    "> As we mentioned before, the recommended first Task in any Workflow is the Advanced Image Preprocessor Task. Define the Advanced Image Preprocessor Task using its Task name, 'AOP_Strip_Processor', and the S3 location of the ordered image as the Task's input port. The Task author has named this input port, 'data'.\n",
    "\n",
    "```python\n",
    "# define the pre-processing Task\n",
    "aop_task = gbdx.Task('AOP_Strip_Processor', data=source_s3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assigning Tasks(s) to a Workflow*\n",
    "\n",
    "> A Workflow can execute many Tasks and facilitate the movement of data between those Tasks. For this simple example, there is only one Task. \n",
    "\n",
    "```python\n",
    "# define the Workflow and pass in aop_task\n",
    "my_workflow = gbdx.Workflow([ aop_task ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Starting and saving a Workflow*\n",
    "\n",
    "> The 'savedata' feature will automatically save the output of this Worflow to your GBDX Customer S3 bucket. Pass in the aop_task output by referencing its output port, also named 'data'. Set the location parameter to the output directory you specified earlier. \n",
    "\n",
    "```python\n",
    "# save the output of the Workflow to S3\n",
    "my_workflow.savedata(aop_task.outputs.data, location=target_s3)\n",
    "```\n",
    "\n",
    "> Execute the Workflow. Once the Workflow is started, the Platform will spin up the compute resources and data to run each Task, and will run until each of the Tasks in the Workflow has completed. \n",
    "\n",
    "```python\n",
    "# execute the Workflow\n",
    "my_workflow.execute()\n",
    "```\n",
    "\n",
    ">Print and save the Workflow ID, as this will allow you to track and manage your Workflows. \n",
    "\n",
    "```python\n",
    "print my_workflow.id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 4.5 Run the code in the following cell to execute the Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the S3 location of the input data\n",
    "source_s3 = gbdx.catalog.get_data_location(catalog_id='10400100245B7800')\n",
    "\n",
    "# define the S3 location of the output directory\n",
    "target_s3 = 'demo_output/aop_10400100245B7800/'\n",
    "\n",
    "# define the pre-processing Task\n",
    "aop_task = gbdx.Task('AOP_Strip_Processor', data=source_s3)\n",
    "\n",
    "# define the Workflow\n",
    "my_workflow = gbdx.Workflow([ aop_task ])\n",
    "\n",
    "# save the output of the Workflow to S3\n",
    "my_workflow.savedata(aop_task.outputs.data, location=target_s3)\n",
    "\n",
    "# execute the Workflow\n",
    "my_workflow.execute()\n",
    "print my_workflow.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0681151b-73f1-4779-ad79-e527a33a7693"
    }
   },
   "source": [
    "## 5. Manage Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a Workflow is started, you can use the workflow object, called 'my_workflow' in this example, to track and manage the Workflow events. As long as you have the Workflow ID, you can always access this information. \n",
    "\n",
    "__ 5.1 Get the status of the Workflow. This will return the status of whichever event is currently running within the Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eb816a1a-6f6f-42a9-a809-5796cd526c94"
    }
   },
   "outputs": [],
   "source": [
    "print my_workflow.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.2 List the events that have taken place or are currently taking place within the Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print json.dumps(my_workflow.events, sort_keys=True, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.3 Get the stdout and stderr for the Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_workflow.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_workflow.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.3 Each Task also has a unique ID. Get the Task IDs. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task_ids = my_workflow.task_ids\n",
    "print task_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.4 Get the stdout and stderr of a specific Task within the Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdout = gbdx.workflow.get_stdout(my_workflow.id, task_ids[0])\n",
    "print stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stderr = gbdx.workflow.get_stderr(my_workflow.id, task_ids[0])\n",
    "print stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.5 Find out if the Workflow completed and succeeded. __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_workflow.complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_workflow.succeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.6 Generate the Workflow JSON schema running behind the scenes. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_workflow.generate_workflow_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> You can cancel a Workflow while it's running.\n",
    "\n",
    "```python\n",
    "my_workflow.cancel()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 5.7 You can also get information about your Workflow later if you've saved the Workflow ID. Replace the Workflow ID in the following call with your Workflow ID. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wf = gbdx.workflow.get('4578270231173558359')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61eda7bc-eed7-4cc5-ab87-ae5a3a06fcbc"
    }
   },
   "source": [
    "## 6. A more complex Workflow \n",
    "\n",
    "The above example Workflow only has one Task assigned to it. The power of the Workflow system, however, is in being able to link outputs between Tasks. In this next example, we will assign the output of the image pre-processing Task as input to the OpenSkyNet (OSN) Task. This Task will extract features from imagery using a trained neural net model. We provide a model that detects aircraft.  \n",
    "\n",
    "In this Workflow, you'll first pre-process the input image as you did in the above example (parameterized for the OSN Task), crop the pre-processed image to a more manageable size, then run the OSN Task on the prepared image.  \n",
    "\n",
    "```python\n",
    "# define the S3 location of the input data\n",
    "source_s3 = gbdx.catalog.get_data_location(catalog_id='103001003A230A00')\n",
    "\n",
    "# define a directory in which to save the OSN Workflow output\n",
    "target_s3 = 'demo_output/osn_103001003A230A00'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You're going to use the Advanced Image Preprocessor Task again, but not the default parameters as before. The OSN Task requires pansharpened, DRA'd, atmospherically compensated, imagery of a specific orthorectification pixel size and interpolation type. Set the input port for this Task,'data', to the image S3 location you defined above.\n",
    "\n",
    "```python\n",
    "# define the pre-processing Task to prepare an image for the OSN Task\n",
    "aop_task = gbdx.Task(\"AOP_Strip_Processor\", data=source_s3, enable_dra=True, enable_pansharpen=True,\n",
    "    enable_acomp=True, ortho_epsg='UTM', bands='PAN+MS', ortho_pixel_size='0.5',\n",
    "    ortho_interpolation_type='Bilinear')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next Task crops the AOP'd image from the previous Task. We use a cropped image because the OSN Task is a compute intensive Task, and may timeout if used on large images. Set the output of aop_task as input to crop_task. \n",
    "\n",
    "```python\n",
    "# define the crop image Task to crop the pre-processed image by a bounding box \n",
    "crop_task = gbdx.Task(\"CropGeotiff\", data=aop_task.outputs.data, output_to_root_dir=True, wkt=\"POLYGON((-77.49189376831055 38.97302269384043,-77.43335723876953 38.97302269384043,-77.43335723876953 38.920688310253,-77.49189376831055 38.920688310253,-77.49189376831055 38.97302269384043))\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, define the OSN Task that will run a trained neural net model to extract aircraft from imagery. Set the crop_task output as osn_task input. Set the model to the aircraft model we've provided, with the recommended parameters.  \n",
    "\n",
    "```python\n",
    "# define the OSN Task using an aircraft model \n",
    "osn_task = gbdx.Task(\"openskynet-v5:0.0.2\", data=croptask.outputs.data, \n",
    "    model='s3://vector-lulc-models/0ad86e8caf6d9000.zip', log_level='trace', confidence='0.85', pyramid=True,\n",
    "    pyramid_window_sizes='[150, 80]', pyramid_step_sizes='[40, 20]', step_size='15', tags='Airliner, Fighter,\n",
    "    Helicopter')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next step is to create a Workflow as you did before, but this time the Workflow will contain more Tasks, with connected inputs and outputs. \n",
    "\n",
    "```python\n",
    "# define the Workflow and pass in aop_task, crop_task, and osn_task \n",
    "my_workflow2 = gbdx.Workflow([ aop_task, crop_task, osn_task ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the savedata feature again to save the Workflow output to your GBDX customer S3 bucket, to the OSN output directory you specified above. \n",
    "\n",
    "```python\n",
    "# save the OSN output to S3  \n",
    "my_workflow2.savedata(osn_task.outputs.data, location=target_s3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# execute the Workflow and print the Workflow ID   \n",
    "my_workflow2.execute()\n",
    "print my_workflow2.id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 6.1 Run the code in the following cell to execute the new Workflow. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the S3 location of the input data\n",
    "source_s3 = gbdx.catalog.get_data_location(catalog_id='103001003A230A00')\n",
    "\n",
    "# define a directory in which to save the OSN Workflow output\n",
    "target_s3 = 'demo_output/osn_103001003A230A00'\n",
    "\n",
    "# define the pre-processing Task to prepare an image for the OSN Task\n",
    "aop_task = gbdx.Task(\"AOP_Strip_Processor\", data=source_s3, enable_dra=True, enable_pansharpen=True,\n",
    "    enable_acomp=True, ortho_epsg='UTM', bands='PAN+MS', ortho_pixel_size='0.5',\n",
    "    ortho_interpolation_type='Bilinear')\n",
    "\n",
    "# define the crop image Task to crop the pre-processed image by a bounding box \n",
    "crop_task = gbdx.Task(\"CropGeotiff\", data=aop_task.outputs.data, output_to_root_dir=True,\n",
    "    wkt=\"POLYGON((-77.49189376831055 38.97302269384043,-77.43335723876953 38.97302269384043,-77.43335723876953 38.920688310253,-77.49189376831055 38.920688310253,-77.49189376831055 38.97302269384043))\")\n",
    "\n",
    "# define the OSN Task using an aircraft model\n",
    "osn_task = gbdx.Task(\"openskynet-v5:0.0.2\", data=crop_task.outputs.data, model='s3://vector-lulc-models/0ad86e8caf6d9000.zip', log_level='trace', confidence='0.85', pyramid=True, pyramid_window_sizes='[150, 80]', pyramid_step_sizes='[40, 20]', step_size='15', tags='Airliner, Fighter, Helicopter')\n",
    "\n",
    "# define the Workflow and pass in aop_task, crop_task, and osn_task \n",
    "my_workflow2 = gbdx.Workflow([ aop_task, crop_task, osn_task ])\n",
    "                              \n",
    "# save the OSN output to S3                              \n",
    "my_workflow2.savedata(osn_task.outputs.results, location=target_s3)\n",
    "\n",
    "# execute the Workflow and print the Workflow ID                              \n",
    "my_workflow2.execute()\n",
    "print my_workflow2.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 6.2 Use the tools we learned earlier to track the Workflow. You'll notice that you'll get an event for each Task, for each change in state.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_workflow2.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print json.dumps(my_workflow2.events, sort_keys=True, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for event in my_workflow2.events:\n",
    "    print event['task'], event['event']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on learning how to search the Catalog, order imagery, and run Workflows. You can see the output of your Workflows by logging into the [S3 browser](http://s3browser.geobigdata.io/) with your GBDX credentials, and navigating the output directories you created at `'demo_output/'`. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "9208e874-c6ae-4644-a337-14318b82306a": {
     "id": "9208e874-c6ae-4644-a337-14318b82306a",
     "layout": "treemap",
     "prev": null,
     "regions": {
      "46d5e71a-43f9-466c-8a6e-fa1990773125": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 3.8,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "b7721a7e-da27-4348-81ee-2e5c1b0f74e0",
        "part": "whole"
       },
       "id": "46d5e71a-43f9-466c-8a6e-fa1990773125"
      }
     }
    },
    "febf44cb-85df-4ca8-8ead-4eace1b1d11e": {
     "id": "febf44cb-85df-4ca8-8ead-4eace1b1d11e",
     "prev": "9208e874-c6ae-4644-a337-14318b82306a",
     "regions": {
      "11fb263f-91cd-4b15-9207-84f97fcb6224": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "eea5f7da-d58a-454c-9d90-d512db58da58",
        "part": "whole"
       },
       "id": "11fb263f-91cd-4b15-9207-84f97fcb6224"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
